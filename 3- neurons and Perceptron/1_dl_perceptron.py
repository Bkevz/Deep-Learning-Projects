# -*- coding: utf-8 -*-
"""1_DL_Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bB0pSm2AOmj-iZl0o3x2dulRNt1aZFl4

#Perceptron

###Importing Libraries
"""

from numpy import array #For Array Initialization
from numpy import random #For Randomly choosing Numbers
from numpy import dot #For Doing DOT Product
from random import choice
"""###Activation Function"""

activationFn = lambda x: 0 if x < 0 else 1 #step activation function (if i/p is negative o/p is 0 else 1)

"""###Training Dataset"""

dataset = [
    (array([0,0,1]), 0), #array([x,y,b],e) x,y=Input , b=bias, e=Expected O/P to validate
    (array([0,1,1]), 1),
    (array([1,0,1]), 1),
    (array([1,1,1]), 1),
]
print(dataset)

"""###Initializing Random numbers for WEIGHTS"""

weights = random.rand(3)

"""###Initializing additional variables"""

r = 0.2 #learning Rate
n = 100 #Number of Iteration

"""###Training our Model"""

for j in range(n):
    x, expected = choice(dataset) #random input set, Includes repeated numbers 
    #print(x,expected)
    result = dot(weights, x) #dot product of the input and the weight vectors
    err = expected-activationFn(result)
    weights += r * err * x #calculate the correction factor, added to weights to improve o/p in next iteration
    #If the expected value is bigger, the weights should be increased, and if expected value is smaller, the weights should be decreased

"""###Evaluating Model"""

for x, _ in dataset:
    result = dot(x, weights)
    print("Input: {}  ResultBAFn: {} ResultAFn: {}".format(x[:2], round(result,3), activationFn(result)))
